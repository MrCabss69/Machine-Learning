{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un oyente atento es todo lo que hace falta para formular buenas preguntas\n",
    "Y un hablante sabio es lo que hace falta para contestarlas. Y atención, atención también hace falta. De eso precisamente va este tutorial de la Arquitectura Transformers, que está detrás del desarrollo de los LLMs, de la atención. ¿Qué es la atención para las máquinas? Como en la mayoría de casos: números. Y en el lenguaje, aunque parezca irónico, no podría ser de otra forma. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La arquitecutura Transformers original propone una solución para el problema de la generación de texto, basada en una arquictura con dos componentes: un codificador, y un decodificador.\n",
    "\n",
    "Por una parte la meta del codificador es traducir el texto recibido en una secuencia de embeddings, es decir: una representación numérica de la secuencia original. Esta representación matemáticamente representa un punto de un espacio $N$-dimensional, siendo N el número de posiciones del array de embeddings generado.\n",
    "\n",
    "El decodificador recibe como input esa representación matemática del texto, y en base a él genera una secuencia de texto de salida. Parte del potencial de esta arquitectura es que se puede aplicar a problemas diversos, desde traducción hasta respuesta de preguntas, pasando por resumen de texto. Potencialmente podemos configurar el modelo para que 'responda' como queramos, en base a la entrada del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
